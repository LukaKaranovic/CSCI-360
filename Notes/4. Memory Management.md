## No Memory Abstraction
![[memoryorganization.png]]
- Three simple ways of organizing an **OS and one user process** in **memory**.
- They aren't possible as you will never have enough memory to give each process on your system its own space.

Without any memory abstraction:
- **Relocation problem** arises with **two user processes**.
![[relocationproblem.png]]
- If they are running independently, we can just load them one after the other in memory from address 0 (shown in a and b)
- If they are running at the same time, they will load one after the other (shown in c)
	- As you can see, process b has instruction `JMP 28`, this won't work in case c because it will jump back to process a code (it needs to actually jump to 16412).
		- Could replace 0 with new starting point: 16384
			- This solution is called dynamic relocation

# Memory Abstraction - Address Space
For each process we are giving them an address space for them to use.
- Every process allows you to **address by byte**
	- Ex. address 5 is the 5th byte in the memory.

Each process's **abstracted address space** always **starts from 0 and ends at its limit**
- **base** and **limit** registers are used to give each process a separate address space.

Each **abstract address** is *translated* to an actual **physical address** by adding the **base value**.

## Allocation to Growing Segments
Processes have portions of memory that are growing and shrinking throughout execution, so we must give each process additional memory space.
- Ex. stack and heap
![[growingsegmentallocation.png]]

# Swapping
If memory is **limited**, all the processes will not get proper memory allocation and **will not run.**

We do know a few things about memory:
- Memory is **needed only** when a **process is running**
- Memory is **not needed** when a **process is waiting** to be rescheduled or for an I/O operation to complete.
Therefore:
- Memory from a **waiting process** can be **swapped** with another process so it can run.
- Memory can be **swapped back** to a process when it is done waiting (rescheduled to run)

![[swappingdiagram.png]]

## External Fragmentation and Compaction
**External Fragmentation** - total memory space exists to satisfy a request, but it is **not contiguous**. 
- Ex. two separate 2KB holes so a 4KB application can't use that space.
- Wastage of memory as free memory is in smaller blocks, meaning only small programs can fill those gaps.
	- Makes it likely that larger programs may never have enough memory space to execute.

Can **reduce** external fragmentation by **compaction**
- Shuffle memory contents to place all free memory **together in one large block** 
- Compaction is possible only if relocation is dynamic, and is done at execution time 
- **Compaction** is CPU intensive and costly, i.e., **not used**

# Free Memory Management
How do we know where the free and used memory spaces are?
- When allocating memory, we are not allocating byte-by-byte, we have some **minimum memory allocation unit** (determined by OS)
- We can use a bitmap to see what intervals are free and used (example below)
![[freememmgmt.png]]
- Note: A terminating process can either
	- create a new hole 
	- merge with older holes

**Internal fragmentation** - some programs won't be as big as the minimum memory allocation unit. Therefore, part of that memory block is unused and wasted.
- Ex. minimum memory allocation unit is 4KB, and a 1KB program is executing
	- It takes that 4KB block, but only 1KB is used

# Memory Allocation Algorithms
**First fit** 
- Allocate the first fit hole
**Next fit** 
- Starts searching from the last found position and allocates the next fit hole
**Best fit** 
- Allocate the "best match" hole
- Slower since it has to check every hole.
**Worst fit**
- Allocate the "worst match" hole
- Slower since it has to check every hole.
- Uses big holes that may not be used frequently in other algorithms
- May not use small holes frequently.
**Quick fit**
- Maintain multiple lists of **most frequently used** size holes and allocate from the best fit list.
	- If hole doesn't fit into any list, goes to misc. list
- When processes leave, they might not just leave a new hole, they may merge with an existing hole. 
	- So maintaining the list requires updating and inserting, not just inserting,

No one algorithm is the best, they all have costs and benefits.

# Virtual Memory
There is a need to run programs that are **too large** to fit in memory.
- Solution is to split programs into little pieces called **overlays**
	- Kept on the disk, **swapped** in and out of memory.

Each program has its own **virtual address space**, broken up into fixed size chunks called **pages**.
- Virtual address space can be larger than the physical RAM and starts from 0.
- A process does not need **all its pages** in memory to run (which is why this works).
- It can run only if **relevant pages** are in memory.

The **memory management unit (MMU)** loads **relevant pages** of a process into memory to run it and **translates** virtual addresses into physical addresses.

# Paging
![[mmu_diagram.png]]

Ex. 64K **virtual** address space with 16 **pages** of size 4096 or 4K
- Every page begins on a multiple of 4096 and ends 4095 addresses higher.
	- Ex. 4K-8K is really 4096-8191
32K **physical** address space with 8 **page frames** of size 4K each.
- A page from a virtual address space can be **loaded** on any empty page frame in physical address space.

These addresses won't match - as there are addresses, say for example 20000, in virtual space that aren't in physical space.

## Page Table
The mapping between virtual pages and physical pages frames is kept in the **page table**.
- Programmers use virtual addresses in code
- Virtual to physical address **translation** is performed by the **MMU** using the **page table**

![[pagetable.png]]
- Every address can be found by using a page number + an offset
	- We can do page number * page size + offset
- In virtual address space, we have 16 pages, so 4 bits to represent the page number, and then 12 bits for offset (2<sup>12</sup> = 4096 or 4K)
- In physical address space, we have 8 page frames, so 3 bits to represent the page number, 12 bits for offset (page frame size still 4K)
- Page table maps the 16 virtual pages to the 8 physical page frames.
	- If a virtual page is mapped to a page frame, its present/absent bit will be 1.
		- Notice that 8 (number of page frames) entries have the bit set to 1.

A typical page table entry:
![[pagetable_entry.png]]

## Paging Issues
- Paging suffers from **internal fragmentation**. Memory less than a page size cannot be allocated.
- Mapping using the page table needs a memory read, which is a slow operation.

## Translation Lookaside Buffers
TLBs, implemented in hardware, can be used to **speed up the mapping**.
- it is a type of cache, may not have all entries but it will work for frequently used pages.

TLB is maintained in hardware, not in memory (like page table).
- Since it is in hardware, there aren't many entries (not much space)
	- Quicker, but not as much information
- Keeps basic information (shown below)
![[tlb.png]]
1. Can we get frame number from TLB?
2. Is it valid or not?
3. If not found, then we have to use page table to make a new entry and also update TLB.

If TLB is full, use some algorithm to replace a TLB entry.

#### Back to Paging
![[Pasted image 20251020130512.png]]

# Page Fault and Page Replacement
When the MMU attempts to translate a virtual address into a physical address and the related page is **not** in the memory a **page fault** occurs

During a **page fault**, MMU brings the **faulty page** from the disk into an **empty page frame** in the memory and updates both page table and TLB. It then completes the translation process with the updated page table and TLB.
- If an empty page frame is **not available,** another page from the memory is **replaced** by MMU to bring the faulty page into memory

If the page selected to be replaced is **written** while in memory, it needs to be **written back to disk** before eviction. 

It is better to **select** a page to be replaced that is **not heavily used** or will not be **required very shortly.** 

There are many algorithms to select a page to replace.

## Optimal Algorithm
The best selection is to select a page that will be **required next** after a very long time

Each page can be labeled with the number of instructions that are going be executed before the next reference of it occurs.
- At page fault, the page with the highest label should be removed.

Unfortunately, while ideal, this strategy is not practical.

## Page Table Entry: R and M Bits
R bit of a page is set when the page **reference** occurs and is **cleared** at every clock interrupt.

M bit of a page is set when the page **write** occurs and is **not cleared** at clock interrupts

## Not Recently Used (NRU) Algorithm
At page fault, system categorizes pages based on the **current values of their R and M bits:**
- Class 0: not referenced, not modified
	- R = 0, M = 0
- Class 1: not referenced, modified
	- R = 0, M = 1
	- M cannot be set to 1 without R being set to 1, but R can be set back to 0 in another clock cycle.
- Class 2: referenced, not modified
	- R = 1, M = 0
- Class 3: referenced, modified
	- R = 1, M = 1

Removes a page at **random** from the **lowest-numbered** non-empty class.
- Not referenced is lower than referenced to avoid having to replace another page in near future
- Not modified is lower than modified to avoid disk writes

**Not very good performance**

## FIFO Algorithm
![[Pasted image 20251014131721.png]]
- System maintains a list of all pages currently in memory
	- Stores most recently arrived page at the tail
	- Stores least recently arrived page (oldest) at the head.
- At page fault, the page at the head is removed and the new page is added to the tail.

Not used in real systems
- Performance isn't very good

## Second-Chance Algorithm
System maintains a list of all pages currently in memory, with the most recent arrival at the tail and the least recent arrival at the head
- Same as FIFO algorithm

At page fault, the page at the headâ€™s R bit is checked.
- If R bit is **zero**, the page is **removed** and the new page added to the tail of the list.
- If R bit is **one**, the page is **removed** from the head and **added back to the tail**, with **R bit cleared** and load time modified.
	- The page is not replaced, it is instead treated like a newly loaded page.
	- The search continues to find a victim (removing head until R bit is zero and it can place new page)

A bit better than FIFO

## Clock Page Replacement Algorithm
Keeps all the pages in memory in a **circular list** in the form of a clock.
![[Pasted image 20251014132741.png]]
- R = 0:
	- Evict current page
	- Load new page
	- Advance clock hand
- R = 1:
	- Clear R (set to 0)
	- Advance clock hand
	- Repeat search

Only maintaining one pointer (clock hand) instead of two (head and tail)
- Otherwise, works the same as the second chance algorithm

## Least Recently Used (LRU) Algorithm
System is equipped with a **64-bit counter** in hardware.
- Counter value is incremented every instruction executed by the **entire system.**
- At every page reference, current counter value is recorded in the corresponding **page table entry.**
	- The higher the counter, the more recently used the page is.
- At page fault, the page with the **lowest** counter value is removed.
	- This will be the page in memory that was used longest ago (least recent)

Looks at history of use, least recently used is least likely to be referenced again.

We don't actually have this counter for the CPU.
- Counting each instruction is quite expensive

## Not Frequently Used (NFU) Algorithm
System maintains a **software counter** for **each page** with initial value zero.
- At every clock interrupt, all the pages in the memory are checked and its R bit (0 or 1), is added to its counter
	- Counter will increment if page was referenced in a clock interrupt
- At page fault, page with the **lowest** counter value is removed

Idea: Rarely used pages are likely to keep being used rarely.

What's bad:
- A newer page that is being frequently used is still going to have a lower counter than infrequently used older pages (aging).

### Simulating LRU in Software
1. System maintains a **software counter** for **each page** with initial value zero.
2. At every clock interrupt, each page counter is shifted 1 bit right and a 1 is added at the left (MSB) if the page has been referenced (R = 1) in this clock tick
	- this is called **aging**
3. At page fault, the page with lowest counter value is removed
![[Pasted image 20251014135318.png]]


## Working Set Algorithm
The idea:
- The **set** of pages that a process is **actively using** at an instance is called its **working set**.
- If the entire working set is in **memory**, the process will run without causing many page faults
- A program causing **page faults every few instructions** is said to be **thrashing**
- Working set algorithm aims to **minimize thrashing**
- Pre-paging the working set into memory may cause less thrashing compared to **demand paging**.
	- However, pre-paging is less practical than demand paging

![[Pasted image 20251014140024.png]]
- The working set is the **set of pages** used by the **k most recent** memory **references**
- The function w(k, t) is the size of working set at time t
	- As k increases, working set increases until a certain value, say k<sub>t</sub>, then it stays flat.

### Working Set Algorithm with Shift Register
System maintains a **shift register of length k.**
- Like a queue

At every memory reference, the register is shifted one position left and the most recently referenced page is inserted on the right (sliding window).
- The set (window) of k pages in the shift register is the working set.

At page fault, a page **not** in the **shift register** is **removed**.

Problem:
- Maintaining the shift register at **every page reference** and processing it at a page fault would be expensive

### Working Set Algorithm with Time of Last Use Approximation
**Current virtual time** of a process is computed by adding its actual CPU usage time with its start time.
- Used to approximate the time of last use of each page of the process
- Recorded in the page table entry (updated not every page reference, but **every page fault**)

At page fault, page table entries are scanned, searches for one page to be evicted.
- If R bit of selected page to be evicted is 1:
	- The current virtual time of the process is **written** as the **time of last use** of the page 
	- **Proceeds**
- If R bit of selected page to be evicted is 0:
	- The **age** of the last use of the page is computed by subtracting its last use time from its current virtual time.
		- If the age is greater than T, the page is evicted, and the **scan continues** to **update** other page entries.
			- T is a threshold value
		- If the age is less than or equal to T, the page is kept and it searches for a new victim
			- the page with the greatest age is kept tracked.
			- If no page with age greater than T is found, the page with the greatest age is evicted
- If there is **no** page with R bit 0 in memory, a page is removed at **random**

![[Pasted image 20251015131403.png]]
- Diagram showing relevant information for working set with time of last use approximation
- Each process has a page table, this is a page table of a process.
	- Current virtual time of this process is 2204
	- The pages of process have different last use times
		- The ones with R bit = 1 will be updated to 2204
		- The **first page** with R bit = 0 that age > T will be removed.
			- If not, R bit 0 page with lowest last use time is removed (largest age)
			- If no R bit 0 pages, a page is removed at **random**
#### Problem
- Every time there is a page fault, we have to update the entire page table for a process.

## WSClock Algorithm
Combines the **working set** and **clock** algorithms.

At page fault, the **page table entry at clock hand** is checked.
**Phase 1 (1 cycle):** check this
- If its R bit is 1:
	- R bit is set to 0
	- The current virtual time of the process is **written** as the **time of last use** of the page
	- Proceeds by advancing clock hand
- If its R bit is 0:
	- Age > T:
		- Checks M bit:
		- If M bit is 0:
			- Page is **evicted** and new page is loaded, time of last use is set to current virtual time.
		- If M bit is 1:
			- Page is **not evicted**, but a **disk write** for the page is **scheduled**
		- Proceeds by advancing clock hand
	- Age <= T:
		- If age is greatest seen so far, it is stored.
		- Proceeds by advancing clock hand
**Phase 2 (1 cycle):**
- If no page has been evicted, the clock hand keeps moving to find a **clean page** to evict
	- Clean page: R and M bits are both 0, AND no pending disk write
	- Note that all pages will have R bit 0 because they are set to 0
- If no clean page has been found, the **page at the clock hand** is evicted.

For now, algorithm ends when a page is replaced.

# Local vs. Global Allocation Policies
Can replace pages locally (within a process) or globally (within all RAM). We have to alter page replacement algorithms based on what we are doing.

![[Pasted image 20251015134459.png]]
- Say we are trying to add page A6 (for process A) using LRU algorithm
- In b) we replace A5 because it has the lowest LRU time within process A (local)
- In c), we replace B3 because it has the lowest LRU time of any page (global)

# Paging Backing Store
Problem:
- What if you have virtual address space of a process that is huge (since virtual space of a process has no limit)?

Paging backing store: Purpose is to act as an extension of RAM
- Handles situations where more memory is needed than is physically available
- Stored on secondary storage (disk)

## Swap Area/Space
Large disk space with the size equal to the size of the virtual address space of a process is allocated to the process.
- This space is called the **swap area**

There is dedicated disk space in the swap area for each page.
- Each page is swapped back and forth between memory and disk using this dedicated disk space
- Swap area **base address** in the disk and the **page number** of the page are used to locate the dedicated disk space for the page

Problem - this requires very large disk space or swap area for a process with a very large virtual address space.

## Disk Map
No disk space is allocated to a process **upfront** 
- No dedicated space in the disk for each page is **required**

The page is **backed up** on the disk on demand 
- The address of the page in the disk is mapped in a table called a **disk map**
- Each page is swapped back and forth between memory and the disk using the disk map

This requires less disk space
- Only the pages backed up on the disk at a particular instance, rather than space for all pages.

# Segmentation
The **address space of a process** has logical divisions as follows: 
1. Code 
2. Data 
3. Constants 
4. Stack 
5. Heap

Instead of using a single and flat address space and dividing it into pages, a process can use multiple independent address spaces called **segments**.
- one segment for each logical division

Why? 
- It is not necessary to place all the segments of a process together in main memory, they can be placed independently anywhere in the memory.
- Different segments of a process can be different sizes.
- Each segment can grow and shrink independently without bumping into each other.
- No internal fragmentation because there is no fixed minimum size for a segment

However:
- Programmers need to be aware of **multiple** segments instead of one big block
- External fragmentation is **unavoidable** because gaps in memory can be too small for a segment
	- Can solve it with compaction (but is not easy or optimal)

![[Pasted image 20251015140927.png]]
- External fragmentation solved by compaction

With segmentation, each **logical address of a process** will have a **segment number** and an **offset**.

Each process will have a **segment table** with **segment descriptors** that will keep track **where each segment starts.**
- Address translation in a segmentation mechanism is very simple:
	- **Segmentation number** is used as the index of the segmentation table to find the start of the segment (**segment base address**)
		- If segment is not in memory, segment table has no segment base address for that segment.
	- **Offset** is added to the segment base address to find the physical address.

Segmentation address translation:
![[segment_address_translation.png]]

### Segmentation with Paging
In segmentation, an **entire segment** must be in memory
- Can't only have part of a segment in memory.
- This is a **problem for large segments**, so we can use **paging with segmentation** to avoid loading the entire segment into memory.
	- Segments will have internal fragmentation again

#### MULTICS Structure for Segmentation with Paging
![[multics_segmnent_table.png]]
- List of segment descriptors

![[Pasted image 20251015144929.png]]
- Each segment descriptor points to a page table for the segment

Virtual address in MULTICS consists of two parts:
1. The segment
2. Address within the segment (page # and offset)

![[multics_address_lookup.png]]
- Gets segment number, looks up segment in segment table
- Goes to page table from segment descriptor
- Looks up page number in page table
- Goes to page and adds offset to find specific address

![[multics_tlb.png]]
- Simplified MULTICS translation lookaside buffer (TLB)

![[Pasted image 20251020131243.png]]

# Multilevel Page Tables
![[Pasted image 20251020132056.png]]
- Gets frame number from second-level page table

Why? 
- Page table is held in memory, if you have a lot of pages, page table will be super big and can't be held in one frame (in RAM when needed)
	- Can split it up into many smaller page tables that can each fit in 1 frame, then put 1 or 2 page tables in RAM at a time.

It's bad because you have to read from memory more times (1 read per page table)

# Inverted Page Tables
![[Pasted image 20251020133327.png]]
- If we don't have enough space in RAM for a big page table, we can turn the space of RAM into a hash table for pages, which each entry having a linked list to solve collisions.
- Only 1GB space, so 2^18 spaces for the 2^52 pages, so some pages will have same hash and will be part of linked list at that hash.
- Tradeoff: the smaller you make your hash table, the longer the linked lists will be and therefore the lookups will be longer on average
