Time multiplexing - use a scheduling algorithm
Space multiplexing - use an allocation algorithm

**Process scheduling** - scheduling the processes in the processor so that all programs are executed in a seamless manner.

# Process Abstraction
- A **process** is an *abstraction* of a **running program**.
- Execution of a program starts via GUI mouse clicks or command line entry of its name
- One program's code can be run in several processes.

- A program becomes a process when the **executable code** is loaded into memory and **starts running**.
- Process execution progresses in **sequential fashion** from the beginning to the end of the code.
- A process has **more parts** other than the code.

A process has the following parts:
- **Text section**: The executable code
- Current activity represented by **program counter (PC)** and **processor registers**
- **Stack**: holds temporary data
	- return addresses, function parameters, and local variables
- **Data section**: holds global variables/constants
- **Heap**: holds dynamically allocated variables during run time

Registers:
- Stack register keeps track of top of stack
	- Needed to find local variables, function calls/returns, and context switches
- Segment register
	- Lots of different CPU registers that tell the processor which segment of memory to use for code, data, stack, etc.
- Data segment registers
	- Contains info about global/static variables (things in data section)

## Process Memory
![[processmemory.png]]

# Process Operations

## Process Creation
**Four** principal events that cause processes to be created:
1. **System Initialization**
2. Execution of a process creation **system call**, `fork()`, by a running process
3. A **user request** to create a new process
4. Initiation of a **batch** job

## Process Termination
Typical conditions which terminate a process:
#### Voluntary terminations:
- **Normal** exit - process finishes successfully and calls an exit routine (like `exit()`, `return 0`)
- **Error** exit - process detects something went wrong and terminates itself with a **nonzero exit code**
#### Involuntary terminations:
- **Fatal error** - process makes an unrecoverable mistake and the **OS kills it immediately**
	- Ex. division by zero, seg fault, illegal instruction
- **Killed** by another process - another process (often OS or a user) explicitly issues a kill command or signal
	- User runs `kill` shell command
	- OS kills processes during shutdown
	- Parent process terminates a child that misbehaved

# Process States
**Three** states a process may be in:
1. **Running** - actually using the CPU at that instant
	- Ex. Process is newly created and scheduler picked it to run
2. **Ready** - runnable; temporarily stopped to let another process run
	- Ex. Process finishes an I/O request and is now ready to compute.
	- Ex. A higher-priority process arrives and current process is sent back to ready
3. **Blocked** - unable to run until some external event happens
	- A process can't wake up itself. The scheduler doesn't know about it
	- When an I/O device finishes response, it gives interrupt to system, now OS knows it is ready.
	- Ex. A process requests a disk read → OS puts it in the **disk wait queue** (Blocked).
		- While it waits, the CPU runs other processes.
		- Disk controller finishes the operation → raises a **hardware interrupt**.
		- Interrupt handler in the OS notes which process was waiting → moves it from the blocked queue to the **ready queue**.
		- Now the scheduler can eventually choose it → process resumes running.
![[processstatesflowchart.png]]

# Process Control Block
Each process is represented in the OS by a **process control block** (data structure), which holds the information/metadata related to the process.

It contains:
- **Process Managements**
	- **Process IDs** - (self, parent, group) 
	- **Process state** - ready, running, blocked 
	- **Program counter** - location of instruction to execute next 
	- **CPU registers** - contents of all process-centric registers
	- **CPU scheduling information** - priorities, scheduling queue pointers 
	- **Accounting information** - CPU used, clock time elapsed since start, time limits 
- **Memory-management information** - Memory allocated to the process (pointers to text, data, and stack segments).
- **File-management information** - root and working directories, list of open files, user and group IDs.

# Threads
A thread is the **smallest unit of execution** (smallest sequence of programmed instructions that can be **managed independently by a scheduler**) within a process, allowing a program to perform multiple tasks concurrently

![[wordprocessorthreads.png]]
- Keyboard for input
- Display/screen for output
- Can read/write to disk to save or open existing files.

We have to do something so that in the same process, all three of these things run as if they are concurrent.
- Otherwise you would only be able to write when you can't see the display. Can't see or write when saving, etc.

![[Pasted image 20250915130358.png]]
- One thread that looks for requests (dispatcher)
- A created 'worker thread' for each request

```c
	// Dispatcher Thread:
	while (true) {
		get_next_request(&buf);
		handoff_work(&buf);
	}

	// Worker Thread:
	while (true) {
		wait_for_work(&buf);
		look_for_page_in_cache(&buf, &page);
		if (page_not_in_cache(&page)) {
			read_page_from_disk(&buf, &page);
		}
		return_page(&page);
	}
```

A thread cannot exist without a process, a thread is a **part** of a process.

## Multithreading
![[Pasted image 20250915131017.png]]
- In a), no multithreading, just several processes
	- Only one instruction executed at a time
- In b) multithreading
	- Several instructions can be executed concurrently, one per thread

## Multi Threaded Process: Shared Items

| Per Process (all threads)   | Per Thread      |
| --------------------------- | --------------- |
| Address space               | Program counter |
| Global variables            | Registers       |
| Open files                  | Stack           |
| Child processes             | State           |
| Pending alarms              |                 |
| Signals and signal handlers |                 |
| Accounting information      |                 |
 
## POSIX Thread
![[Pasted image 20250915131722.png]]
- How to do multithreading in programming (GNU/Linux, C)

![[Pasted image 20250915132123.png]]
- Each thread runs through `thread_function()`
- `pthread_create()` passes a pointer to make the thread object there
	- NULL for default attribute of thread
	- we want this thread to run `thread_function()`
		- the function we want to thread run has to return a `void*` and it has to have a `void*` parameter for the thread (and no other params)
		- `void*` means that any data type pointer is accepted
- `pthread_exit()` is terminating all threads before they leave `thread_function()`

# Process Scheduling - Why?
- Modern OS allows **multiple processes** even on a **single CPU.** 
- CPUs are **time-shared** among the processes. 
- A **process scheduler** shares the CPUs among the processes in a seamless way. 
- Maximum CPU utilization obtained with multiprocessing

## Process Execution Cycle
- Process execution consists of a two-burst cycle of **CPU execution** and **I/O wait**
	- or CPU burst followed by I/O burst
	- CPU burst distribution is the main concern with scheduling
- A process with mainly long CPU bursts is a **CPU-bound process**
- A process with mainly long I/O bursts is an **I/O-bound process**

**Process scheduler** maintains **scheduling queues** of processes:
- **Ready queue** – set of all processes residing in main memory, ready and waiting to execute 
	- Scheduler removes process from this queue when process is scheduled
- **Device queues or I/O queues** – set of processes waiting for an I/O device 
	- Each device has its own queue (Ex. keyboard queue, disc queue)
	- An I/O scheduler for that device (part of device driver) removes process from this queue, does required actions, sends interrupt which signals OS to move it to the ready queue
**Process scheduler** selects among available processes from **ready queue** for next execution on CPU 
- How this is handled depends on the scheduling algorithm
Processes migrate among the various queues

## Preemptive vs Non-preemptive Scheduling
**Non-preemptive** CPU scheduling decisions may take place when a process: 
1. Switches from running to waiting state 
2. Switches from running to ready state 
3. Switches from waiting to ready 
4. Terminates 
- Can't prevent process from doing rest of work until it is either done or voluntarily yields the CPU.

**Preemptive** CPU scheduling decisions may take place:
1. Upon expiration of the time slice of a process
2. When an interrupt occurs
- Can prevent process from doing rest of work by interrupting it and switching the CPU to a different process.

## Queueing Diagram
![[Pasted image 20250915135245.png]]

## Dispatcher
**Dispatcher** module gives the control of the CPU to the process selected by the scheduler; this involves: 
- switching context 
- switching to user mode 
- jumping to the proper location in the user program to restart that program 
**Dispatch latency** – the time it takes for the dispatcher to stop one process and start another running
- If context switching involves a lot of things, dispatcher latency will be longer

## Context Switching
- When CPU switches to another process, the system must **save the state** of the old process and **load the saved state** for the new process via a **context switch** 
- **Context** of a process is represented in the PCB (process control block)
![[Pasted image 20250915135714.png]]
- Context-switch time is overhead; the system does no useful work while switching (dispatcher latency goes up)
	- The more complex the OS and the PCB -> the longer the context switch
- Time dependent on hardware support 
	- Some hardware provides multiple sets of registers per CPU -> multiple contexts loaded at once

We need to **optimize context switching**, meaning we shouldn't **swap processes** in our schedule **frequently**

# Process Scheduling - How?
We have three categories of algorithms:
1. Batch
	- A system that processes a group of jobs or tasks in a sequence without human intervention.
2. Interactive
	- A system that has a specified response time
3. Real time
	- A system that has a deadline for a response
	- We aren't focused on real time scheduling in this course

## Process Scheduling Algorithm Goals
*All Systems* 
- **Fairness**: giving each process a fair share of the CPU
- **Policy Enforcement**: seeing that the stated policy is carried out
- **Balance**: Keeping all parts of the system busy
*Batch Systems* 
- **Throughput**: maximize jobs per hour 
- **Turnaround time**: minimize time between submission and termination 
- **CPU utilization**: keep the CPU busy all the time 
	- minimize context switching and idle time
*Interactive Systems* 
- **Response time**: respond to requests quickly
- **Proportionality**: meet users’ expectation 
*Real-time Systems* 
- **Meeting deadlines**: avoid losing data 
- **Predictability**: avoid quality degradation in multimedia systems

## Batch Systems Process Scheduling

### First-Come First-Serve (FCFS)
Processes are ran in the order they arrive in, one after another.

| Process Arrival Order | Burst Time |
| --------------------- | ---------- |
| P1                    | 24         |
| P2                    | 3          |
| P3                    | 3          |
- Waiting time for P1 = 0; P2 = 24; P3 = 27
- Average waiting time = (0 + 24 + 27)/3 = 17
- Turnaround time for P1 = 24; P2 = 27; P3 = 30
- Average turnaround time: (24 + 27 + 30)/3 = 27

If shorter processes arrive before longer ones, the times are much lower.
- **Convoy effect** - short process behind long process
	- Consider one CPU-bound and many I/O-bound processes

### Shortest Job First (SJF)
Associate each process with the **length** of its next **CPU burst** and use these lengths to schedule the process with the **shortest burst.**

| Process | Burst TIme |
| ------- | ---------- |
| P1      | 6          |
| P2      | 8          |
| P3      | 7          |
| P4      | 3          |
Order is P4 -> P1 -> P3 -> P2
- Average waiting time = (3 + 16 + 9 + 0)/4 = 7
- Average turnaround time = (9 + 24 + 16 + 3)/4 = 13

**SJF is optimal** – gives minimum average waiting and average turnaround time for a given set of processes 
- The difficulty is knowing the length of the next CPU burst

### Shortest Remaining Time First (SRTF)
SRTF adds the concepts of **varying arrival times** and **preemption** to scheduling.

| Process | Arrival Time | Burst |
| ------- | ------------ | ----- |
| P1      | 0            | 8     |
| P2      | 1            | 4     |
| P3      | 2            | 9     |
| P4      | 3            | 5     |
Order: ![[srtfganttchart.png]]
- Average waiting time = \[(0-0)+(1-1)+(17-2)+(5-3)]/4 = 17/4 = 4.25 
- Average turnaround time = \[(17–0)+(5-1)+(26-2)+(10-3)]/4 = 52/4 = 13

Cost is that there is **more context switching** as it is **preemptive**.

## Interactive System Process Scheduling

### Priority (PR)
A **priority number** (integer) is associated with each process 
The CPU is allocated to the process with the highest priority (smallest integer = highest priority) 
- Can be either preemptive or non-preemptive (depends on implementation)
- Preemptive 
	- Priority of job takes precedence even if another job is currently running
- Non-preemptive 
	- If a job is currently running, it is safe to finish even if a higher priority job arrived

*Note: SJF is priority scheduling where priority is the inverse of predicted next CPU burst time*

Problem: **Starvation** – low priority processes may never execute 

Solution: **Aging** – as time progresses increase the priority of the process

| Process | Burst Time | Priority |
| ------- | ---------- | -------- |
| P1      | 10         | 3        |
| P2      | 1          | 1        |
| P3      | 2          | 4        |
| P4      | 1          | 5        |
| P5      | 5          | 2        |
Order: P2 -> P5 -> P1 -> P3 -> P4
- Average waiting time (6+0+16+18+1)/5 = 41/5 = 8.2
- Average turnaround time (16+1+18+19+6)/5 = 60/5 = 12

### Round Robin (RR)
Each process gets a small unit of CPU time **(time quantum q),** usually 10-100 milliseconds. After this time has elapsed, the process is **preempted** and added to the end of the **ready queue**.
- If there are n processes in the ready queue and the time quantum is q, then each process gets 1/n of the CPU time in chunks of at most q time units at once. 
	- Mathematically, no process waits more than (n-1)q time units
- Timer **interrupts** every quantum to schedule next process
- Performance:
	- If q is large -> FIFO (queue)
	- If q is small -> bad, q must be large with respect to context switch, otherwise the overhead from lots of context switching is too high

#### Example

| Process | Burst Time |
| ------- | ---------- |
| P1      | 24         |
| P2      | 3          |
| P3      | 3          |
Time quanta = 4 msec
Order: ![[rrganttchart.png]]
- Average waiting time (0+4+7) = 11/3 = 3.67 
- Average turnaround time (30+7+10) = 47/3 = 15.67
- Typically, higher average turnaround than SJF, but better **response/waiting time** (user perception)
- q should be large compared to context switch time 
- q usually 10ms to 100ms, context switch < 10 usec

**80% of CPU bursts should be shorter than q**
- **not equal to it**

### Multiple Queue
So far, all of our algorithms are applied to a singular ready queue.

In multiple queue scheduling, the process **ready queue** is partitioned into separate queues.
- Foreground (interactive)
- Background (batch)

Processes are permanently in a given queue
Each queue has its own scheduling algorithms
- Foreground -> RR
- Background -> FCFS

Scheduling must be done between the queues:
- **Fixed priority scheduling:** (i.e. serve all from foreground and then from background).
	- Possibility of starvation
- **Time slice** - each queue gets a certain amount of CPU time which it can schedule among its processes (i.e. 80% to foreground in RR and 20% to background in FCFS).

You can have more than two queues, for example:
![[manyqueuescheduling.png]]
- This diagram is assuming you're using fixed priority to schedule the queues

### Shortest Predicted Next (SPN)

Lot of algorithms depend on CPU bursts, but it's hard to get the CPU burst time
- Some lines of code are more intensive than others
- Don't know how many iterations a loop will run for

Our goal is to predict the length of a CPU burst, then pick the process with the shortest predicted next CPU burst.
- Can be done by using the length of previous CPU bursts, using exponential averaging
	1. t<sub>n</sub> = actual length of nth CPU burst
	2. T<sub>n+1</sub> = predicted value for the next CPU burst
	3. a, 0 <= a <= 1
	4. T<sub>n+1</sub> = a * tn + (1 - a)T<sub>n</sub>
	- **a is often set to 1/2**
	- we can set some common value for t<sub>0</sub> if we have no previous data to work off of
![[spngraph.png]]

#### Values of a
- a = 0
	- Tn+1 = T<sub>n</sub>
	- In this case, everything but most recent CPU burst does not count
- a = 1
	- T<sub>n+1</sub> = at<sub>n</sub>
	- In this case, only the actual last CPU burst counts
- If we expand the formula, we get:
	- T<sub>n+1</sub> = at<sub>n</sub> + (1-a)t<sub>n-1</sub> + ... + (1-a)<sup>j</sup>t<sub>n-j</sub> + ... + (1-a)<sup>n+1</sup>T<sub>0</sub>
- Since both a and (1-a) are <= 1, each successive term has less weight than its predecessor (because it's raised to higher powers)

### Guaranteed Scheduling (GS)
In **guaranteed scheduling**, if **n processes** are running, each one is entitled to get **1/n** of the CPU cycles. 
- Keeps track of how many CPU cycles each process has had since its creation.
- Computes the **ratio** of actual CPU time consumed to CPU time entitled to. 
- Runs the process with the **lowest ratio** until its ratio has moved above that of its closest competitor

### Lottery Scheduling (LS)
- **Lottery scheduling** gives processes **lottery tickets** for CPU time. 
- Whenever a scheduling decision has to be made, a lottery ticket is **chosen at random,** and the process holding that ticket gets the CPU. 
- Scheduler might hold a lottery 50 times a second, with each winner getting 20 msec of CPU time as a prize. 
- More important processes can be given extra tickets, to increase their odds of winning. 
- A process holding a **fraction f** of the tickets will get about a **fraction f of the CPU share.**

### Fair-Share Scheduling (FSS)
**Fair-share scheduling** takes into account which **user owns** a process before scheduling it. 
- Each user is allocated some fraction of the CPU. 
- Scheduler picks processes in such a way as to enforce the share.
- If two users have each been promised 50% of the CPU, they will each get that, no matter how many processes they have in existence.
#### Example
Say user 1 is running processes A, B, C, and D. User 2 is running only one process E
- With round-robin scheduling:
	- A B C D E | A B C D E | A B C D E ...
- With fair-share scheduling, and both users get 50%:
	- A E | B E | C E | D E | A E ...
- With fair-share scheduling, but user 1 gets 67% of CPU time and user 2 gets only 33%:
	- A B E | C D E | A B E ...
